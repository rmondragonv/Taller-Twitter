{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1K9KRhCzhzvU"
      },
      "source": [
        "import pandas as pd\n",
        "import tweepy\n",
        "\n",
        "# Leer las llaves\n",
        "\n",
        "keys = pd.read_csv(\"mis_llaves.csv\", header=None)\n",
        "keys = dict(zip(keys[0],keys[1]))\n",
        "keys.keys()\n",
        "\n",
        "auth = tweepy.OAuthHandler(keys['api_key'], keys['api_secret_key'])\n",
        "auth.set_access_token(keys['access_token'], keys['access_token_secret'])\n",
        "api = tweepy.API(auth, wait_on_rate_limit=True)\n",
        "\n",
        "from tweepy import Stream\n",
        "from tweepy.streaming import StreamListener\n",
        "\n",
        "class MyListener(StreamListener):\n",
        "\n",
        "def on_data(self,data):\n",
        "try:\n",
        "with open(\"python.json\",\"a\") as f\n",
        "f.write(data)\n",
        "return true\n",
        "except BaseException as e:\n",
        "print (\"Error on_data: %s\" str(e))\n",
        "return True\n",
        "\n",
        "def on_error(self,status):\n",
        "print(status)\n",
        "return True\n",
        "\n",
        "twitter_stream = Stream(auth,MyListener())\n",
        "twitter stream.filter(track=[\"#python\"])\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "tweet = \"RT @juank: la clave del exito es la academia\"\n",
        "print(word_tokenize(tweet))\n",
        "\n",
        "\n",
        "import re\n",
        "emoticons_str = r\"\"\"\n",
        "(?:\n",
        "[:=;] #Eyes\n",
        "[oO\\-] #Nose (Optional)\n",
        "[D\\)\\]\\(\\]/\\\\0pP #Mouth\n",
        ")\"\"\"\n",
        "regex_str =[\n",
        "emoticons_str,\n",
        "r\"@juank: la clave del exito es la academia\" , #HTML tags\n",
        "r\"@juank: la clave del exito es la academia\" , #@-Mención\n",
        "r\"@juank: la clave del exito es la academia\" , #Hash-tags\n",
        "r'https[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f]\n",
        "[0-9a-f]))+' . #URLs\n",
        "r\"@juank: la clave del exito es la academia\"' , #Números\n",
        "rjuank: la clave del exito es la academia\" , #Palabras con -y'\n",
        "r\"@juank: la clave del exito es la academia\" , #Otras Palabras\n",
        "r\"@juank: la clave del exito es la academia\" #Otras Palabras\n",
        "]\n",
        "\n",
        "tokens_re = re.compile (r'('+'|'.join(regex_str)+')' ,\n",
        "re.VERBOSE | re.IGNORECASE)\n",
        "emoticon_re = re.compile(r'^'+emoticons_str+'$', re-VERBOSE |\n",
        "re.IGNORECASE)\n",
        "def tokenize(s):\n",
        "return tokens_re.findall(s)\n",
        "def preprocess(s, lowercase=False):\n",
        "tokens = tokenize(s)\n",
        "lowercase:\n",
        "tokens = [token if emoticon_re.search(toke)\n",
        "else token.lower() for token in tokens]\n",
        "return tokens\n",
        "tweet = \"@juank: la clave del exito es la academia\"\n",
        "print (preprocess (tweet))\n",
        "\n",
        "import re \n",
        "\n",
        "pattern1 = '?P<pic>pic.twitter.com/[^\\s]+'\n",
        "pattern2 = '?P<url>https?://[^\\s]+'\n",
        "\n",
        "def text_clean(row):\n",
        "    text = row['tweets']\n",
        "    \n",
        "links = [tuple(j for j in i if j)[-1] for i in re.findall(f\"({pattern1})|({pattern2})\",text)]\n",
        "for link in links:\n",
        "text = text.replace(link,\"\")\n",
        "    \n",
        "hashtags = [interaction for interaction in text.split() if interaction.startswith(\"#\")]\n",
        "for hashtag in hashtags:\n",
        "text = text.replace(hashtag, \"\")\n",
        "        \n",
        "mentions = [interaction for interaction in text.split() if interaction.startswith(\"@\")]\n",
        "for mention in mentions:\n",
        "text = text.replace(mention, \"\")\n",
        "        \n",
        "return text, links, hashtags, mentions\n",
        "  \n",
        "  \n",
        "data[['texto_limpio', 'links', 'hashtags', 'mentions']] = data.apply(text_clean, axis=1, result_type='expand')\n",
        "data\n",
        "\n",
        "with open (\"mytweets.json\" ,\"r\") as f:\n",
        "for line in f:\n",
        "tweet = json.loads(line)\n",
        "tokens = preprocess(tweet[\"text\"])\n",
        "do_something_else(tokens)\n",
        "\n",
        "import operator\n",
        "import json\n",
        "from collections import Counter\n",
        "fname = \"mytweets.json\"\n",
        "with open(fname,\"r\") as f:\n",
        "      count_all = Counter()\n",
        "      for line in f:\n",
        "          tweet = json.loads(line)\n",
        "          #Crea una lista con todos los términos\n",
        "          terms_all = [term for termn in preprocess (tweet[\"@juank: la clave del exito es la academia\"])]\n",
        "          #Actualiza el contador\n",
        "          count_all.update(terms_all)\n",
        "          #Imprime las primeras 5 palabras con mayor frecuencia\"\n",
        "          print(count_all.most_common(5))\n",
        "\n",
        "terms_stop = [term for term in preprocess(tweet[\"text\"])\n",
        "                if term not in stop]\n",
        "\n",
        "#Contar una sóla vez un término, equivalente a la frecuencia de documento\"\n",
        "\n",
        "terms_single = set(terms_all)\n",
        "#Contar sólamente los hashtag\"\n",
        "terms_hash = [term for term in preprocess(tweet[\"text\"])\n",
        "               if term.startswith(\"#\")]\n",
        "#Contar sólo los terminos (no hashtags, no menciones)\"\n",
        "terms_only = [term for term in preprocess(tweet[\"text\"])\n",
        "               if term not in stop and\n",
        "              not term.startswith((\"#\" ,\"@\"))]\n",
        "\n",
        "\n",
        "import regex\n",
        "import emoji\n",
        "\n",
        "def get_emojis(text):\n",
        "    emoji_list = []\n",
        "    data = regex.findall(r'\\X', text)\n",
        "    for word in data:\n",
        "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
        "            emoji_list.append(word)\n",
        "\n",
        "    return emoji_list\n",
        "  \n",
        "  \n",
        "\n",
        "data['emojis'] = data['texto_limpio'].apply(lambda text: get_emojis(text))\n",
        "data.head()\n",
        "\n",
        "\n",
        "data['hora'] = data['date'].dt.floor('T').dt.time\n",
        "temp = pd.DataFrame(data.hora.value_counts()).reset_index()\n",
        "temp.columns = ['hora', 'cnt']\n",
        "temp = temp.sort_values('hora')\n",
        "temp\n",
        "\n",
        "\n",
        "import plotly.io as pio\n",
        "import plotly.express as px\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
        "\n",
        "def get_tweets(start_id, parameters):\n",
        "    \"\"\"\n",
        "    Generador para buscar tweets.\n",
        "    :param start_id: `int`. Identificador del tweet mas antiguo a buscar\n",
        "    search API only returns up to around a week of history\n",
        "    :param parameters: diccionario con los parametros de busqueda\n",
        "    \"\"\"\n",
        "    latest_tweets = api.GetSearch(**parameters)\n",
        "    if not len(latest_tweets):\n",
        "        return []\n",
        "    last_id = latest_tweets[-1].id\n",
        "    for tweet in latest_tweets:\n",
        "        yield tweet\n",
        "    while last_id >= start_id:\n",
        "        parameters['max_id'] = last_id - 1\n",
        "        results = api.GetSearch(**parameters)\n",
        "        if len(results):\n",
        "            for tweet in results:\n",
        "                yield tweet\n",
        "            last_id = results[-1].id\n",
        "            print(f'last seen: {last_id} @ {results[-1].created_at}')\n",
        "        else:\n",
        "            break\n",
        "\n",
        "tweets = []\n",
        "for tweet in get_tweets(start_id, parameters):\n",
        "    tweets.append(tweet)\n",
        "\n",
        "    import plotly.io as pio\n",
        "import plotly.express as px\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "init_notebook_mode(connected=True)\n",
        "\n",
        "\n",
        "trace = go.Scatter(x=temp.hora.values,\n",
        "                   y=temp.cnt.values,\n",
        "                   text=[f\"Fecha: {f}<br>Tuits: {c}\" for f,c in zip(temp.hora.values,temp.cnt.values)],\n",
        "                   hoverinfo='text',\n",
        "                   mode='lines+markers',\n",
        "                   name='Horas',\n",
        "                   line={'color': 'blue'})\n",
        "\n",
        "layout = go.Layout(title=\"Número de tuits por hora\")\n",
        "\n",
        "fig = go.Figure(data=[trace], layout=layout)\n",
        "iplot(fig)\n",
        "\n",
        "import scipy.cluster.hierarchy as sch\n",
        "import numpy as np\n",
        "\n",
        "pairwise_distances = sch.distance.pdist(temp)\n",
        "linkage = sch.linkage(pairwise_distances, method='ward')\n",
        "idx_to_cluster_array = sch.fcluster(linkage, pairwise_distances.max() * 0.5, criterion='distance')\n",
        "idx = np.argsort(idx_to_cluster_array)\n",
        "temp = temp.copy()\n",
        "    \n",
        "temp2 = temp.iloc[idx, :].T.iloc[idx, :]\n",
        "my_idx = idx_to_cluster_array\n",
        "\n",
        "temp2.index = ['tweet #'+str(i) for i in temp2.columns]\n",
        "temp2.columns = temp2.index\n",
        "\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "def extract(url):\n",
        "    elem = None\n",
        "    driver = webdriver.Firefox()\n",
        "    driver.get(url)\n",
        "\n",
        "    try:\n",
        "        found = WebDriverWait(driver, 10).until(\n",
        "            EC.visibility_of(\n",
        "                driver.find_element(By.TAG_NAME, \"article\")\n",
        "            )\n",
        "        )\n",
        "        # copia de los datos relevantes, porque Selenium arrojará si\n",
        "        # acceder a las propiedades después de que el controlador \n",
        "        elem = {\n",
        "          \"text\": found.text\n",
        "        }\n",
        "    finally:\n",
        "        driver.close()\n",
        "\n",
        "    return elem\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}